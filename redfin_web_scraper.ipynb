{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "---\n",
    "\n",
    "Code in this notebook is used to pull data from multiple house listings from Redfin. Selenium is used to log in to Redfin and download price history data (user will need to ender their own Redfin username/password). \n",
    "\n",
    "To keep track of which house listings have already been scraped, a dictionary of URLs is maintained. Additional URLs are pulled from each webpage and added to the full dictionary of URLs.\n",
    "\n",
    "Webpages are scraped in batches (user to enter batch size) to minimize data loss if any errors occur or if the scraping must be paused.\n",
    "\n",
    "Note that only a small sample is saved in the scraped_data folder. User will need to determine how much data they need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T16:39:23.451470Z",
     "start_time": "2019-04-06T16:39:23.447597Z"
    }
   },
   "source": [
    "# Table of Contents\n",
    "---\n",
    "0. [Import Packages](#import_packages)\n",
    "\n",
    "1. [Create Functions to Pull Data From Redfin](#create_functions)\n",
    "  * [Set Up Webdriver and Helper Functions to Pull From Individual Webpages](#set_up_webdriver_function)\n",
    "  * [Pull Listing Details From Individual Webpages](#pull_listing_details_function)\n",
    "  * [Add URLs from Similar Houses to Dictionary of All URLS to Pull](#add_to_urls_function)\n",
    "  * [Pull Data From List of URLs](#pull_data_function)\n",
    "  \n",
    "2. [Extract Data](#extract_data)\n",
    "  * [A. Open Existing Dictionary of URLs to Scrape Or Create New URLs Dictionary](#open_urls_dict)\n",
    "  * [B. Add more URLs from Redfin Searches (Optional)](#add_urls)\n",
    "  * [C. View How Many Links Are Unscraped in the List of URLs](#view_urls)\n",
    "  * [D. Start Scraping and Saving Data](#pull_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Import Packages  <a name=\"import_packages\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T22:13:16.114363Z",
     "start_time": "2019-04-06T22:13:16.109585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Webscraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Store data\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Create Functions to Pull Data From Redfin  <a name=\"create_functions\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Webdriver and Helper Functions to Pull From Individual Webpages <a name=\"set_up_webdriver_function\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T02:02:17.288065Z",
     "start_time": "2019-04-07T02:02:17.281851Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_driver(a_url=None, incognito=False):\n",
    "    \"\"\"\n",
    "    Opens an instance of Chrome using Selenium, and returns the driver.\n",
    "    Requires that chromedriver is installed in Applications folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a_url: string, optional, default None\n",
    "        Loads the URL, if provided.\n",
    "\n",
    "    incognito: boolean, optional, default False\n",
    "        Opens Chrome in incognito mode if True.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set options for webdriver\n",
    "    option = webdriver.ChromeOptions()\n",
    "    if incognito:\n",
    "        option.add_argument(\" â€” incognito\")\n",
    "\n",
    "    # Create webdriver\n",
    "    driver = webdriver.Chrome(\n",
    "        executable_path='/Applications/chromedriver', options=option)\n",
    "\n",
    "    # Opens URL if provided\n",
    "    if a_url:\n",
    "        driver.get(a_url)\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def soup_from_driver(a_driver):\n",
    "    soup = BeautifulSoup(a_driver.page_source, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def clean_from_text(text):\n",
    "    \"\"\"\n",
    "    Helper function to clean text.\n",
    "    Add additional cleaning functions as necessary.\n",
    "    \"\"\"\n",
    "    cleaned_text = text.strip()\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def open_xpath_with_driver(xpath, a_driver):\n",
    "    \"\"\"\n",
    "    Helper function to use Selenium to click items\n",
    "    on the webpage (e.g., Javascript). Item location\n",
    "    is indicated by xpath. \n",
    "    \"\"\"\n",
    "    a_driver.find_element_by_xpath(xpath).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T17:25:00.377133Z",
     "start_time": "2019-04-06T17:25:00.374379Z"
    }
   },
   "source": [
    "### Pull Listing Details From Individual Webpages <a name=\"pull_listing_details_function\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T02:02:17.780857Z",
     "start_time": "2019-04-07T02:02:17.756564Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_address(a_soup, a_dict, a_driver):\n",
    "    \"\"\"\n",
    "    Pulls address information from webpage.\n",
    "\n",
    "    Data is pulled from a_soup and added to all the previously\n",
    "    scraped data in a_dict (data first stored in temporary dict).\n",
    "    a_driver is used to get the URL of the page scraped \n",
    "    (URL is used as key for a_dict).\n",
    "    \"\"\"\n",
    "\n",
    "    # Temporary dictionary to store address data\n",
    "    address_dict = defaultdict(str)\n",
    "\n",
    "    # Get addresses and save in address_dict\n",
    "    for line in a_soup.find(itemprop='address').find_all('span'):\n",
    "        if line.has_attr('itemprop'):\n",
    "            address_dict[clean_from_text(\n",
    "                line['itemprop'])] = clean_from_text(line.text)\n",
    "\n",
    "    # Get latitude/longitude and save in address_dict\n",
    "    for line in a_soup.find(itemprop='geo').find_all('meta'):\n",
    "        if line.has_attr('itemprop'):\n",
    "            address_dict[clean_from_text(line['itemprop'])] = float(\n",
    "                line['content'])\n",
    "\n",
    "    # Adds address information from address_dict into a_dict\n",
    "    a_dict[a_driver.current_url] = {\n",
    "        **a_dict[a_driver.current_url], **address_dict}\n",
    "\n",
    "\n",
    "def get_key_details(a_soup, a_dict, a_driver):\n",
    "    \"\"\"\n",
    "    Pulls main info table from webpage (underneath photos). This includes\n",
    "    Public Details (Condo/Coop, Garage, etc.), County, Building,\n",
    "    Community, and Year Built. \n",
    "\n",
    "    Data is pulled from a_soup and added to all the previously\n",
    "    scraped data in a_dict (data first stored in temporary dict).\n",
    "    a_driver is used to get the URL of the page scraped \n",
    "    (URL is used as key for a_dict).\n",
    "    \"\"\"\n",
    "    key_details_dict = defaultdict(str)\n",
    "\n",
    "    # Try statement used in case webpage does not have key data table\n",
    "    try:\n",
    "        for div in a_soup.find(class_='keyDetailsList').find_all('div'):\n",
    "            key_details_dict[clean_from_text(div.contents[0].text)] = clean_from_text(\n",
    "                div.contents[1].text)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    a_dict[a_driver.current_url] = {\n",
    "        **a_dict[a_driver.current_url], **key_details_dict}\n",
    "\n",
    "\n",
    "def get_listing_details(a_soup, a_dict, a_driver):\n",
    "    \"\"\"\n",
    "    Pulls Property Details table from webpage. This includes Listing Information,\n",
    "    such as, Listing Price, Target List Date, Property Type, # of \n",
    "    Bedrooms, etc.\n",
    "\n",
    "    Data is pulled from a_soup and added to all the previously\n",
    "    scraped data in a_dict (data first stored in temporary dict).\n",
    "    a_driver is used to get the URL of the page scraped \n",
    "    (URL is used as key for a_dict).\n",
    "    \"\"\"\n",
    "    list_details_dict = defaultdict(str)\n",
    "    for super_group in a_soup.find_all(class_='super-group-content'):\n",
    "        for data in super_group.find_all('span', {'class': 'entryItemContent'}):\n",
    "            try:\n",
    "                list_details_dict[clean_from_text(data.contents[0].string)] = \\\n",
    "                    clean_from_text(data.contents[1].text)\n",
    "            except:\n",
    "                list_details_dict[clean_from_text(\n",
    "                    data.contents[0].string)] = True\n",
    "    a_dict[a_driver.current_url] = {\n",
    "        **a_dict[a_driver.current_url], **list_details_dict}\n",
    "\n",
    "\n",
    "def get_home_facts(a_soup, a_dict, a_driver):\n",
    "    \"\"\"\n",
    "    Pulls Home Facts table from webpage. This includes number of\n",
    "    Beds, Baths, Finished Sq. Ft., Unfinished Sq. Ft., etc.\n",
    "\n",
    "    Data is pulled from a_soup and added to all the previously\n",
    "    scraped data in a_dict (data first stored in temporary dict).\n",
    "    a_driver is used to get the URL of the page scraped \n",
    "    (URL is used as key for a_dict).\n",
    "    \"\"\"\n",
    "    home_facts_dict = defaultdict(str)\n",
    "    for row in a_soup.find(class_='facts-table'):\n",
    "        home_facts_dict[clean_from_text(row.span.text)] = \\\n",
    "            clean_from_text(row.div.text)\n",
    "    a_dict[a_driver.current_url] = {\n",
    "        **a_dict[a_driver.current_url], **home_facts_dict}\n",
    "\n",
    "\n",
    "def get_transit_scores(a_soup, a_dict, a_driver):\n",
    "    \"\"\"\n",
    "    Pulls Transportation table from webpage. This includes \n",
    "    Walk Score, Transit Score, and Bike Score.\n",
    "\n",
    "    Data is pulled from a_soup and added to all the previously\n",
    "    scraped data in a_dict (data first stored in temporary dict).\n",
    "    a_driver is used to get the URL of the page scraped \n",
    "    (URL is used as key for a_dict).\n",
    "    \"\"\"\n",
    "    transit_scores_dict = defaultdict(str)\n",
    "\n",
    "    # Try statement used in case transit score not available\n",
    "    for score in a_soup.find_all(class_='score'):\n",
    "        try:\n",
    "            transit_scores_dict[clean_from_text(score.find(\n",
    "                class_='label').text)] = \\\n",
    "                    clean_from_text(score.find(class_='percentage').text)\n",
    "        except:\n",
    "            None\n",
    "    a_dict[a_driver.current_url] = {\n",
    "        **a_dict[a_driver.current_url], **transit_scores_dict}\n",
    "\n",
    "\n",
    "def get_recent_area_offer_data(a_soup, a_dict, a_driver):\n",
    "    \"\"\"\n",
    "    Pulls Real Estate Sales (Last 30 days) table from webpage. \n",
    "    This includes Median List Price, Median $/Sq.Ft., etc.\n",
    "\n",
    "    Data is pulled from a_soup and added to all the previously\n",
    "    scraped data in a_dict (data first stored in temporary dict).\n",
    "    a_driver is used to get the URL of the page scraped \n",
    "    (URL is used as key for a_dict).\n",
    "    \"\"\"\n",
    "    recent_area_offer_data_dict = defaultdict(str)\n",
    "    recent_area_offer_data_dict['current area'] = clean_from_text(\n",
    "        a_soup.find(class_='OfferInsights').find('a').text)\n",
    "    for td in a_soup.find(class_='OfferInsights').find(class_='basic-table').tbody.find_all('td'):\n",
    "        recent_area_offer_data_dict['Area Current ' + clean_from_text(td.find(class_='field').text)] =\\\n",
    "            clean_from_text(td.find(class_='value').text)\n",
    "    a_dict[a_driver.current_url] = {\n",
    "        **a_dict[a_driver.current_url], **recent_area_offer_data_dict}\n",
    "\n",
    "\n",
    "def get_schools(a_dict, a_driver):\n",
    "    \"\"\"\n",
    "    Pulls Schools and Great School Ratings from webpage. \n",
    "    a_driver used to click through Elementary,\n",
    "    Middle, and High school tabs.\n",
    "\n",
    "    Data is pulled from a_soup and added to all the previously\n",
    "    scraped data in a_dict (data first stored in temporary dict).\n",
    "    a_driver is used to get the URL of the page scraped \n",
    "    (URL is used as key for a_dict).\n",
    "    \"\"\"\n",
    "    xpaths = ['//*[@id=\"schools-scroll\"]/div/div[1]/div/div[1]/div[1]/button', '//*[@id=\"schools-scroll\"]/div/div[1]/div/div[1]/div[2]/button',\n",
    "              '//*[@id=\"schools-scroll\"]/div/div[1]/div/div[1]/div[3]/button', '//*[@id=\"schools-scroll\"]/div/div[1]/div/div[1]/div[4]/button']\n",
    "    types = ['serving this home', 'elementary', 'middle', 'high']\n",
    "\n",
    "    # Try statement used in case not all tabs or data available\n",
    "    for i, xpath in enumerate(xpaths):\n",
    "        open_xpath_with_driver(xpath, a_driver)\n",
    "        a_soup = soup_from_driver(a_driver)\n",
    "        for td in a_soup.find(class_='schools-content').find('table').tbody.find_all('td'):\n",
    "            try:\n",
    "                a_dict['school name'].append(clean_from_text(\n",
    "                    td.find(class_='school-name').text))\n",
    "                rating_list = td.find(class_='gs-rating-row').text.split(\":\")\n",
    "                a_dict['great schools rating'].append(\n",
    "                    clean_from_text(rating_list[1]))\n",
    "                a_dict['type'].append(types[i])\n",
    "                a_dict['url'].append(a_driver.current_url)\n",
    "            except:\n",
    "                None\n",
    "\n",
    "\n",
    "def get_price_history(a_soup, a_dict, a_driver):\n",
    "    \"\"\"\n",
    "    Pulls Price History from webpage. \n",
    "\n",
    "    Data is pulled from a_soup and added to all the previously\n",
    "    scraped data in a_dict (data first stored in temporary dict).\n",
    "    a_driver is used to get the URL of the page scraped \n",
    "    (URL is used as key for a_dict).\n",
    "    \"\"\"\n",
    "\n",
    "    # Try statement used in case data not available\n",
    "    for i, row in enumerate(a_soup.find(id='property-history-transition-node').table.tbody.find_all('tr')):\n",
    "        try:\n",
    "            a_dict['date'].append(clean_from_text(\n",
    "                row.find(class_='date-col').text))\n",
    "            a_dict['event'].append(clean_from_text(\n",
    "                row.find(class_='event-col').findChildren()[0].text))\n",
    "            a_dict['source'].append(clean_from_text(\n",
    "                row.find(class_='source-info').text))\n",
    "            a_dict['price'].append(clean_from_text(\n",
    "                row.find(class_='price-col').text))\n",
    "            a_dict['url'].append(a_driver.current_url)\n",
    "        except:\n",
    "            None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add URLs from Similar Houses to Dictionary of All URLS to Pull <a name=\"add_to_urls_function\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T02:02:18.172243Z",
     "start_time": "2019-04-07T02:02:18.168435Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_new_urls(a_soup, a_driver, a_urls_dict):\n",
    "    \"\"\"\n",
    "    Pulls URLs of similar homes from webpage to add to the \n",
    "    list of URLS to scrape.\n",
    "\n",
    "    Data is pulled from a_soup and added to all the previously\n",
    "    scraped data in a_urls_dict. a_driver is used to get the \n",
    "    URL of the page scraped (URL is used as key for a_dict).\n",
    "    \"\"\"\n",
    "    for row in a_soup.find_all(class_='SimilarHomeCardReact'):\n",
    "        a_urls_dict[\"https://www.redfin.com\"+row.find('a', href=True)['href']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data From List of URLs <a name=\"pull_data_function\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T02:02:19.294478Z",
     "start_time": "2019-04-07T02:02:19.281299Z"
    }
   },
   "outputs": [],
   "source": [
    "def start_pulling_data(urls_dict, num_urls_to_visit):\n",
    "    \"\"\"\n",
    "    Provided a dictionary of urls to visit (urls_dict) and\n",
    "    the number of urls to to visit, this function will log into\n",
    "    Redfin (account needed for full price history), create empty\n",
    "    dictionaries to store data, and download data using get_data.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Create dictionaries to store scraped data\n",
    "        house_data_dict = defaultdict(dict)\n",
    "        price_history_dict = defaultdict(list)\n",
    "        schools_dict = defaultdict(list)\n",
    "\n",
    "        # Filter for unscraped URLS\n",
    "        all_urls = list(filter(lambda x: not urls_dict[x], urls_dict.keys()))\n",
    "\n",
    "        # Pull only DC data\n",
    "        all_urls = [url for url in all_urls if '.com/DC/' in url]\n",
    "\n",
    "        # Make sure number of URLs to visit not larger than total list length\n",
    "        if num_urls_to_visit < len(all_urls):\n",
    "            all_urls = all_urls[:num_urls_to_visit]\n",
    "\n",
    "        # Create driver and log-in\n",
    "        driver = create_driver(all_urls[0])\n",
    "        time.sleep(2)  # Time for page to load\n",
    "        open_xpath_with_driver(\n",
    "            \"//button[@class='button Button compact tertiary-alt']\", driver)\n",
    "        time.sleep(1)  # Time for log-in to load\n",
    "        open_xpath_with_driver(\n",
    "            \"//button[contains(@class, 'emailSignInButton')]\", driver)\n",
    "        time.sleep(1)  # Time for email log-in to load\n",
    "        driver.find_element_by_xpath(\n",
    "            \"//*[@name='emailInput']\").send_keys('XXXXXXXXXXXXXXXX')\n",
    "        driver.find_element_by_xpath(\n",
    "            '//*[@name=\"passwordInput\"]').send_keys('XXXXXXXXXXXXXXXX')\n",
    "        open_xpath_with_driver(\n",
    "            \"//*[contains(concat(' ', @class, ' '), ' button Button primary submitButton ')]\", driver)\n",
    "        time.sleep(3)  # insure enough time for webpage to load\n",
    "\n",
    "        # Start pulling from each webpage now that you are logged in\n",
    "        for url in all_urls:\n",
    "            print(url)\n",
    "            get_data(url, house_data_dict, price_history_dict,\n",
    "                     schools_dict, driver, urls_dict)\n",
    "            urls_dict[url] = True\n",
    "\n",
    "        return house_data_dict, price_history_dict, schools_dict, driver, urls_dict\n",
    "    except:\n",
    "        None\n",
    "\n",
    "\n",
    "def get_data(starting_url, house_data_dict, price_history_dict, schools_dict, driver, urls_dict):\n",
    "    \"\"\"\n",
    "    Uses selenium to get open full price history, then calls \n",
    "    individual functions to pull data from tables on wepage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(starting_url)\n",
    "\n",
    "        # Time for full webpage to load\n",
    "        delay = 5\n",
    "        time.sleep(delay)\n",
    "\n",
    "        # Expands price history\n",
    "        history_xpath = '//*[@id=\"propertyHistory-expandable-segment\"]/div[2]/div/span'\n",
    "        try:\n",
    "            myElem = WebDriverWait(driver, delay).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, history_xpath)))\n",
    "            myElem.click()\n",
    "        except TimeoutException:\n",
    "            print(\"Loading took too much time!\")\n",
    "\n",
    "        # Pulls information from each data table\n",
    "        try:\n",
    "            soup = soup_from_driver(driver)\n",
    "            get_address(soup, house_data_dict, driver)\n",
    "            get_key_details(soup, house_data_dict, driver)\n",
    "            get_listing_details(soup, house_data_dict, driver)\n",
    "            get_home_facts(soup, house_data_dict, driver)\n",
    "            get_transit_scores(soup, house_data_dict, driver)\n",
    "            get_recent_area_offer_data(soup, house_data_dict, driver)\n",
    "            get_schools(schools_dict, driver)\n",
    "            get_price_history(soup, price_history_dict, driver)\n",
    "            get_new_urls(soup, driver, urls_dict)\n",
    "        except:\n",
    "            None\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Extract Data  <a name=\"extract_data\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Open Existing Dictionary of URLs to Scrape Or Create New URLs Dictionary<a name=\"open_urls_dict\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T22:28:17.234191Z",
     "start_time": "2019-04-06T22:28:17.229252Z"
    }
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Open existing dict of URLs if available, otherwise,\n",
    "# create a new dict of URLs from an export of listings from\n",
    "# Redfin (search page export).\n",
    "#\n",
    "# The dictionary keys are URLs, and values are booleans\n",
    "# indicating whether or not the links have been scraped.\n",
    "###########################################################\n",
    "\n",
    "try:\n",
    "    # Change this file to the latest urls_dict file (in scraped_data\n",
    "    # file) if scraping in multiple batches\n",
    "    file = open(\"initial_data/urls_dict.pkl\", 'rb')\n",
    "    old_urls_dict = pickle.load(file)\n",
    "    file.close()\n",
    "except:\n",
    "    old_urls_dict = defaultdict(bool)\n",
    "    new_urls_df = pd.read_csv('initial_data/redfin_search1.csv')\n",
    "    for url in new_urls_df['URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)']:\n",
    "        old_urls_dict[url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Add more URLs from Redfin Searches (Optional)<a name=\"add_urls\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T22:28:17.787257Z",
     "start_time": "2019-04-06T22:28:17.772532Z"
    }
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Add to old_urls_dict with of listings from\n",
    "# Redfin (search page export).\n",
    "#\n",
    "# The dictionary keys are URLs, and values are booleans\n",
    "# indicating whether or not the links have been scraped.\n",
    "###########################################################\n",
    "\n",
    "try:\n",
    "    new_urls_df = pd.read_csv('initial_data/redfin_search2.csv')\n",
    "    for url in new_urls_df['URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)']:\n",
    "        old_urls_dict[url]\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. View How Many Links Are Unscraped in the List of URLs<a name=\"view_urls\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T22:28:18.072275Z",
     "start_time": "2019-04-06T22:28:18.067335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 706 unscraped URLs.\n",
      "706 of the unscraped URLs are in DC.\n"
     ]
    }
   ],
   "source": [
    "view_new_urls = [i for i in old_urls_dict if old_urls_dict[i] == False]\n",
    "view_new_dc_urls = [url for url in view_new_urls if '.com/DC/' in url]\n",
    "print(\"There are {} unscraped URLs.\".format(len(view_new_urls)))\n",
    "print(\"{} of the unscraped URLs are in DC.\".format(len(view_new_dc_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Start Scraping and Saving Data<a name=\"pull_data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T22:32:26.303986Z",
     "start_time": "2019-04-06T22:30:48.200758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.redfin.com/DC/Washington/700-New-Hampshire-Ave-NW-20037/unit-1417/home/40395253\n",
      "http://www.redfin.com/DC/Washington/913-25th-St-NW-20037/home/9043791\n",
      "http://www.redfin.com/DC/Washington/730-24th-St-NW-20037/unit-605/home/40447764\n",
      "http://www.redfin.com/DC/Washington/2522-I-St-NW-20037/home/9043021\n",
      "http://www.redfin.com/DC/Washington/2700-Virginia-Ave-NW-20037/unit-605/home/12534758\n",
      "Loading took too much time!\n",
      "http://www.redfin.com/DC/Washington/2401-H-St-NW-20037/unit-814/home/9046921\n",
      "http://www.redfin.com/DC/Washington/730-24th-St-NW-20037/unit-503/home/143291059\n",
      "http://www.redfin.com/DC/Washington/730-24th-St-NW-20037/unit-511/home/18444824\n",
      "http://www.redfin.com/DC/Washington/700-New-Hampshire-Ave-NW-20037/unit-1114/home/12059612\n",
      "http://www.redfin.com/DC/Washington/922-24th-St-NW-20037/unit-719/home/12534370\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# Creates Chrome instance to scrape\n",
    "# num_sites_to_scrape_per_instance sites per Chrome instance.\n",
    "# This is to batch scraping in case you need to close your\n",
    "# computer or pause the operation.\n",
    "#\n",
    "# Data is pickled in separate files ending with the number\n",
    "# of the scraping instance. In the data cleaning file,\n",
    "# all of the pickled files will be concatenated together.\n",
    "###########################################################\n",
    "starting_instance_num = 0\n",
    "ending_instance_num = 1\n",
    "num_sites_to_scrape_per_instance = 10\n",
    "\n",
    "\n",
    "for i in range(starting_instance_num, ending_instance_num):\n",
    "    try:\n",
    "        house_data_dict, price_history_dict, schools_dict, driver, urls_dict = \\\n",
    "            start_pulling_data(old_urls_dict, num_sites_to_scrape_per_instance)\n",
    "\n",
    "        filehandler = open(\n",
    "            \"scraped_data/house_data_dict{}.pkl\".format(i), \"wb\")\n",
    "        pickle.dump(house_data_dict, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        filehandler = open(\n",
    "            \"scraped_data/price_history_dict{}.pkl\".format(i), \"wb\")\n",
    "        pickle.dump(price_history_dict, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        filehandler = open(\"scraped_data/schools_dict{}.pkl\".format(i), \"wb\")\n",
    "        pickle.dump(schools_dict, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        filehandler = open(\"scraped_data/urls_dict{}.pkl\".format(i), \"wb\")\n",
    "        pickle.dump(urls_dict, filehandler)\n",
    "        filehandler.close()\n",
    "\n",
    "        old_urls_dict = urls_dict\n",
    "    except:\n",
    "        None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
